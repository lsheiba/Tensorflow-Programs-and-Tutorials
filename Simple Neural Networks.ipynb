{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll be introducing neural networks and looking at how to classify digits with these machine learning models. As with all supervised learning methods, we have some (input, output) pairs which we denote as $ (x_i, y_i) $ and we have $n$ of these, so $i \\in [1...n]$. We want to learn a function $f: x \\rightarrow{} y$ that maps inputs to outputs. \n",
    "\n",
    "Today, we're going to look at classifying digits. The inputs will be a 28 x 28 black and white image, and the output will be a 10 dimensional vector that represents the probabilities for each of the 10 digits. From these probabilities, you can then choose the probability with the highest value to produce a single value that best representing the number in the image. \n",
    "\n",
    "![caption](Data/MNISTNN.png)\n",
    "\n",
    "The dataset we're going to be using is called MNIST. MNIST is a dataset of over 60,000 handwritten digits and the corresponding labels of the digits the image represents. It is easily the most used dataset in machine learning. We'll start this tutorial by using the Tensorflow API to load in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(os.environ['DATA_DIR']+os.sep+'MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = 10 # The number of categories the model is choosing from\n",
    "inputSize = 784 # A 28x28 image will have 784 total pixel values\n",
    "numHiddenUnits = 50 # Number of hidden units this one layer NN will have\n",
    "trainingIterations = 10000 # Number of times the training loop is run\n",
    "batchSize = 100 # Represents how many images we feed in one training batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll start by defining placeholders for our inputs and outputs. The purpose of a placeholder is basically to tell Tensorflow \"We're going to input in our input data and labels vector later, but for now, we're going to define these placeholder variables instead\". It lets Tensorflow know about the size of the inputs and labels beforehand. The shape of X will be None x inputSize and None x numClasses for y (our labels). The None keyword means that the value can be determined at session runtime. We normally have None as our first dimension so that we can have variable batch sizes (With a batch size of 100, the input to the network would be 100 x 784). With the None keywoard, we don't have to specify batch_size until later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Just to make sure that we start with a new graph\n",
    "X = tf.placeholder(tf.float32, shape = [None, inputSize])\n",
    "y = tf.placeholder(tf.float32, shape = [None, numClasses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define the weight matrices and bias terms. The key when defining these variable is to keep track of the shapes and to make sure that the output matrix of the previous layer is able to multiply with the weight matrix of the next layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([inputSize, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\n",
    "W2 = tf.Variable(tf.truncated_normal([numHiddenUnits, numClasses], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1), [numClasses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define the matrix multiplication operations and follow each matrix multiply with a non linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hiddenLayerOutput = tf.matmul(X, W1) + B1\n",
    "hiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\n",
    "finalOutput = tf.matmul(hiddenLayerOutput, W2) + B2\n",
    "finalOutput = tf.nn.relu(finalOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Data/NNStructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the above image, the final 10 dimensional output will be fed through a softmax layer to obtain probabilities. Instead of defining the softmax function explicity, we'll be using Tensorflow's function tf.nn.softmax_cross_entropy_with_logits function to perform the softmax + the cross entropy loss all in one line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how the input gets fed through the network, we can analyze the output. The output is a vector that represents the probabilities for the classes. We then want to compare this prediction with the label, and then minimize the loss using a Tensorflow optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = finalOutput))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .1).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the variables that help with calculating accuracy. The correct prediction variable will take a look at the output vector, find the probability with the highest value, and return its index. This is then compared with the actual lables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(finalOutput,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the part where we start training our model. We'll load in our training set by calling Tensorflow's mnist.train.next_batch() function. The sess.run function has two arguments. The first is called the \"fetches\" argument. It defines the value for you're interested in computing/running. For example, in our case, we want to both run the optimizer object so that the cross entropy loss gets minimized and evaluate the current loss value (in case we want to print that value). Therefore, we'll use [opt, loss] for our first argument. The second argument is where we input our feed_dict. This data structure is where we provide values to all of our placeholders. We repeat this for a set number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "step 1000, training accuracy 0.83\n",
      "step 2000, training accuracy 0.96\n",
      "step 3000, training accuracy 0.97\n",
      "step 4000, training accuracy 0.98\n",
      "step 5000, training accuracy 0.99\n",
      "step 6000, training accuracy 0.97\n",
      "step 7000, training accuracy 0.97\n",
      "step 8000, training accuracy 0.99\n",
      "step 9000, training accuracy 0.97\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(trainingIterations):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    batchInput = batch[0]\n",
    "    batchLabels = batch[1]\n",
    "    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n",
    "    if i%1000 == 0:\n",
    "        trainAccuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        print (\"step %d, training accuracy %g\"%(i, trainAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will test the network on data that it has never seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.9675999879837036\n"
     ]
    }
   ],
   "source": [
    "testInputs = mnist.test.images\n",
    "testLabels = mnist.test.labels\n",
    "acc = accuracy.eval(session=sess, feed_dict = {X: testInputs, y: testLabels})\n",
    "print(\"testing accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the accuracy we get when we define a two layer neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.17\n",
      "step 1000, training accuracy 0.97\n",
      "step 2000, training accuracy 0.98\n",
      "step 3000, training accuracy 1\n",
      "step 4000, training accuracy 0.99\n",
      "step 5000, training accuracy 1\n",
      "step 6000, training accuracy 0.99\n",
      "step 7000, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "testing accuracy: 0.9740999937057495\n"
     ]
    }
   ],
   "source": [
    "numHiddenUnitsLayer2 = 100\n",
    "trainingIterations = 10000\n",
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, inputSize])\n",
    "y = tf.placeholder(tf.float32, shape = [None, numClasses])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([inputSize, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\n",
    "W2 = tf.Variable(tf.random_normal([numHiddenUnits, numHiddenUnitsLayer2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1), [numHiddenUnitsLayer2])\n",
    "W3 = tf.Variable(tf.random_normal([numHiddenUnitsLayer2, numClasses], stddev=0.1))\n",
    "B3 = tf.Variable(tf.constant(0.1), [numClasses])\n",
    "\n",
    "hiddenLayerOutput = tf.matmul(X, W1) + B1\n",
    "hiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\n",
    "hiddenLayer2Output = tf.matmul(hiddenLayerOutput, W2) + B2\n",
    "hiddenLayer2Output = tf.nn.relu(hiddenLayer2Output)\n",
    "finalOutput = tf.matmul(hiddenLayer2Output, W3) + B3\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = finalOutput))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .1).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(finalOutput,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(trainingIterations):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    batchInput = batch[0]\n",
    "    batchLabels = batch[1]\n",
    "    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        print (\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "testInputs = mnist.test.images\n",
    "testLabels = mnist.test.labels\n",
    "acc = accuracy.eval(session=sess, feed_dict = {X: testInputs, y: testLabels})\n",
    "print(\"testing accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stanford's Neural Network [Tutorial](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n",
    "* Michael Neilson's [Online Book](http://neuralnetworksanddeeplearning.com/)\n",
    "* Andrej Karpathy's [Blog Post](http://karpathy.github.io/neuralnets/ )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
